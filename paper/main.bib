@article{Wu_Pan_Chen_Long_Zhang_Yu_2021,
  abstractnote = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
  author       = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  doi          = {10.1109/TNNLS.2020.2978386},
  issn         = {2162-2388},
  journal      = {IEEE transactions on neural networks and learning systems},
  month        = {Jan},
  number       = {1},
  pages        = {4–24},
  title        = {A Comprehensive Survey on Graph Neural Networks},
  volume       = {32},
  year         = {2021}
}


@article{Veličković_Cucurull_Casanova_Romero_Liò_Bengio_2018,
  abstractnote = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  author       = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  journal      = {arXiv:1710.10903 [cs, stat]},
  month        = {Feb},
  note         = {arXiv: 1710.10903},
  title        = {Graph Attention Networks},
  url          = {http://arxiv.org/abs/1710.10903},
  year         = {2018}
}


@article{Kipf_Welling_2017,
  abstractnote = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.},
  author       = {Kipf, Thomas N. and Welling, Max},
  journal      = {arXiv:1609.02907 [cs, stat]},
  month        = {Feb},
  note         = {arXiv: 1609.02907},
  title        = {Semi-Supervised Classification with Graph Convolutional Networks},
  url          = {http://arxiv.org/abs/1609.02907},
  year         = {2017}
}

@article{wiki_laplacian,
  journal = {Wikipedia},
  title   = {{Laplacian matrix}},
  url     = {https://en.wikipedia.org/wiki/Laplacian_matrix}
}

@article{wiki_incidence,
  journal = {Wikipedia},
  title   = {{Incidence matrix}},
  url     = {https://en.wikipedia.org/wiki/Incidence_matrix}
}

@article{wiki_clique,
  journal = {Wikipedia},
  title   = {{Clique}},
  url     = {https://en.wikipedia.org/wiki/Clique_(graph_theory)}
}

@misc{gcn_implementation,
  title = {{My implementation of Graph Convolutional Network in Rust programming language}},
  url   = {https://github.com/NChechulin/graph-convolutional-network}
}

@misc{cora_dataset,
  title = {{The CORA dataset}},
  url   = {https://graphsandnetworks.com/the-cora-dataset/}
}

@misc{karate_club_dataset,
  title = {{Zachary's karate club dataset}},
  url   = {http://www.konect.cc/networks/ucidata-zachary/}
}

@book{dig_book,
  author    = {Ma, Yao and Tang, Jiliang},
  doi       = {10.1017/9781108924184},
  place     = {Cambridge},
  publisher = {Cambridge University Press},
  title     = {Deep Learning on Graphs},
  year      = {2021}
}

@misc{3clique_experiment,
  title = {{3-clique merge experiment}},
  url   = {https://github.com/NChechulin/GNN-project/blob/master/experiments/3clique/3clique.ipynb}
}