@misc{3clique_ins_experiment,
  title = {{3-clique merge with insertion experiment}},
  url   = {https://github.com/NChechulin/GNN-project/blob/master/experiments/3clique/3clique_merge.ipynb}
}


@misc{cora_dataset,
  title = {{The CORA dataset}},
  url   = {https://graphsandnetworks.com/the-cora-dataset/}
}


@book{dig_book,
  author    = {Ma, Yao and Tang, Jiliang},
  doi       = {10.1017/9781108924184},
  place     = {Cambridge},
  publisher = {Cambridge University Press},
  title     = {Deep Learning on Graphs},
  year      = {2021}
}

@misc{gcn_implementation,
  title = {{My implementation of Graph Convolutional Network in Rust programming language}},
  url   = {https://github.com/NChechulin/graph-convolutional-network}
}

@misc{karate_club_dataset,
  title = {{Zachary's karate club dataset}},
  url   = {http://www.konect.cc/networks/ucidata-zachary/}
}

@article{Kipf_Welling_2017,
  abstractnote = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.},
  author       = {Kipf, Thomas N. and Welling, Max},
  journal      = {arXiv:1609.02907 [cs, stat]},
  month        = {Feb},
  note         = {arXiv: 1609.02907},
  title        = {Semi-Supervised Classification with Graph Convolutional Networks},
  url          = {http://arxiv.org/abs/1609.02907},
  year         = {2017}
}

@article{simplifying_gcn,
  abstractnote = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
  author       = {Wu, Felix and Zhang, Tianyi and Souza Jr., Amauri Holanda de and Fifty, Christopher and Yu, Tao and Weinberger, Kilian Q.},
  journal      = {arXiv:1902.07153 [cs, stat]},
  month        = {Jun},
  note         = {arXiv: 1902.07153},
  title        = {Simplifying Graph Convolutional Networks},
  url          = {http://arxiv.org/abs/1902.07153},
  year         = {2019}
}

@article{Veličković_Cucurull_Casanova_Romero_Liò_Bengio_2018,
  abstractnote = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  author       = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  journal      = {arXiv:1710.10903 [cs, stat]},
  month        = {Feb},
  note         = {arXiv: 1710.10903},
  title        = {Graph Attention Networks},
  url          = {http://arxiv.org/abs/1710.10903},
  year         = {2018}
}

@article{wiki_clique,
  journal = {Wikipedia},
  title   = {{Clique}},
  url     = {https://en.wikipedia.org/wiki/Clique_(graph_theory)}
}

@article{wiki_incidence,
  journal = {Wikipedia},
  title   = {{Incidence matrix}},
  url     = {https://en.wikipedia.org/wiki/Incidence_matrix}
}

@article{wiki_laplacian,
  journal = {Wikipedia},
  title   = {{Laplacian matrix}},
  url     = {https://en.wikipedia.org/wiki/Laplacian_matrix}
}

@article{Wu_Pan_Chen_Long_Zhang_Yu_2021,
  abstractnote = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
  author       = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  doi          = {10.1109/TNNLS.2020.2978386},
  issn         = {2162-2388},
  journal      = {IEEE transactions on neural networks and learning systems},
  month        = {Jan},
  number       = {1},
  pages        = {4–24},
  title        = {A Comprehensive Survey on Graph Neural Networks},
  volume       = {32},
  year         = {2021}
}

@article{benchmarking_gnns,
  author    = {Vijay Prakash Dwivedi and
               Chaitanya K. Joshi and
               Thomas Laurent and
               Yoshua Bengio and
               Xavier Bresson},
  title     = {Benchmarking Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2003.00982},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.00982},
  eprinttype = {arXiv},
  eprint    = {2003.00982},
  timestamp = {Sat, 23 Jan 2021 01:14:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-00982.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{how_powerful_are_gnns,
  author    = {Keyulu Xu and
               Weihua Hu and
               Jure Leskovec and
               Stefanie Jegelka},
  title     = {How Powerful are Graph Neural Networks?},
  journal   = {CoRR},
  volume    = {abs/1810.00826},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.00826},
  eprinttype = {arXiv},
  eprint    = {1810.00826},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-00826.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

